{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l-6OwywDALtN",
    "outputId": "28a4a700-7581-4f60-c469-56843d6c30a4"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "from astropy.io import fits\n",
    "from astropy.visualization import ImageNormalize, LinearStretch, ZScaleInterval\n",
    "from matplotlib import pyplot as plt\n",
    "from astropy.stats import sigma_clip\n",
    "import numpy as np\n",
    "import os\n",
    "from astropy.wcs import WCS\n",
    "from reproject import reproject_exact\n",
    "from astropy.stats import sigma_clipped_stats\n",
    "#from photutils.profiles import RadialProfile\n",
    "#from photutils.aperture import CircularAperture, aperture_photometry, CircularAnnulus, ApertureStats\n",
    "from astropy.stats import sigma_clipped_stats\n",
    "#from photutils.detection import DAOStarFinder\n",
    "#from photutils.centroids import centroid_1dg\n",
    "\n",
    "#code taken from assignment 2 (ccd reduction)-- we used Dylan's code for reduction since we did it collectively as a group.\n",
    "def create_median_bias(bias_list, median_bias_filename):\n",
    "    # Creates a list of 2d numpy arrays from all of the files in bias_list\n",
    "    bias_images = []\n",
    "    for img_file in bias_list:\n",
    "        bias_data = fits.getdata(img_file)\n",
    "        bias_images.append(bias_data.astype('f4'))\n",
    "\n",
    "    bias_stack = np.stack(bias_images, axis=0)\n",
    "\n",
    "    # Sigma clips the data inside each 2d numpy array\n",
    "    bias_images_masked = sigma_clip(bias_stack, cenfunc='median', sigma=3.0, axis=0)\n",
    "\n",
    "    '''\n",
    "    # Sigma clips the data inside each 2d numpy array\n",
    "    bias_images_masked = sigma_clip(bias_images, cenfunc='median', sigma=3.0, axis=0)\n",
    "    '''\n",
    "    median_bias = (np.ma.median(bias_images_masked, axis=0)).data\n",
    "\n",
    "    # Creates a new file of the median bias\n",
    "    primary = fits.PrimaryHDU(data=median_bias, header=fits.Header())\n",
    "    hdul = fits.HDUList([primary])\n",
    "    hdul.writeto(median_bias_filename, overwrite=True)\n",
    "\n",
    "    # Returns the new median bias frame as a 2d numpy array\n",
    "    return median_bias\n",
    "\n",
    "from astropy.io import fits\n",
    "from astropy.visualization import ImageNormalize, LinearStretch, ZScaleInterval\n",
    "from matplotlib import pyplot as plt\n",
    "from astropy.stats import sigma_clip\n",
    "import numpy as np\n",
    "\n",
    "def create_median_dark(dark_list, bias_filename, median_dark_filename):\n",
    "    # Initialize lists of regular dark data and dark data without bias\n",
    "    dark_images = []\n",
    "    bias_data = fits.getdata(bias_filename)\n",
    "\n",
    "    for file in dark_list:\n",
    "        # Open data in dark files\n",
    "        dark = fits.open(file)\n",
    "        dark_data = dark[0].data.astype('f4')\n",
    "        dark_header = dark[0].header\n",
    "        exptime = dark_header['EXPTIME']\n",
    "\n",
    "        # Subtract bias\n",
    "        dark_data_no_bias = dark_data - bias_data\n",
    "\n",
    "        # Update dark_images with dark current\n",
    "        dark_images.append(dark_data_no_bias / exptime)\n",
    "\n",
    "    # Sigma-clip the dark frames by sigma 3\n",
    "    dark_sc = sigma_clip(dark_images, cenfunc='median', sigma=3, axis=0)\n",
    "\n",
    "    # Combine dark frames into single dark\n",
    "    median_dark = np.ma.mean(dark_sc, axis=0)\n",
    "\n",
    "    # Save combined dark frame to disk\n",
    "    # Set the EXPTIME to 1 second.\n",
    "    dark_hdu = fits.PrimaryHDU(data=median_dark.data, header=dark[0].header)\n",
    "    dark_hdu.header['EXPTIME'] = 1\n",
    "    dark_hdu.header['COMMENT'] = 'Combined dark image with bias subtracted'\n",
    "    dark_hdu.header['BIASFILE'] = (str(bias_filename), 'Bias image used to subtract bias level')\n",
    "\n",
    "    # Save the dark image to disk\n",
    "    dark_hdu.writeto(median_dark_filename, overwrite=True)\n",
    "    # See code in create_median_bias for how to create a new FITS file\n",
    "    # from the resulting median dark frame.\n",
    "\n",
    "    return median_dark\n",
    "\n",
    "from astropy.io import fits\n",
    "from astropy.visualization import ImageNormalize, LinearStretch, ZScaleInterval\n",
    "from matplotlib import pyplot as plt\n",
    "from astropy.stats import sigma_clip\n",
    "import numpy as np\n",
    "\n",
    "def create_median_flat(\n",
    "    flat_list,\n",
    "    bias_filename,\n",
    "    median_flat_filename,\n",
    "    dark_filename=None,\n",
    "):\n",
    "    # Initialize lists for 2d flat numpy arrays and for filter and exposure time of each flat\n",
    "    filter_type = []\n",
    "    flat_images = []\n",
    "    flat_exptime = []\n",
    "    for file in flat_list:\n",
    "          flat = fits.open(file)\n",
    "          filter_type.append(flat[0].header[\"FILTER\"])\n",
    "          flat_exptime.append(flat[0].header[\"EXPTIME\"])\n",
    "          flat_data = fits.getdata(file)\n",
    "          flat_images.append(flat_data.astype('f4'))\n",
    "\n",
    "    # Creates a boolean to check if all filters are the same\n",
    "    same_filter = all(elem == filter_type[0] for elem in filter_type)\n",
    "\n",
    "    # If the filters are not the same then a string detailing the issue is returned\n",
    "    if (not same_filter):\n",
    "          return 'Error, not all flats are from the same filter'\n",
    "\n",
    "    # Subtracts bias data from flat data\n",
    "    bias_data = fits.getdata(bias_filename)\n",
    "    flat_no_bias = []\n",
    "    for data in flat_images:\n",
    "        flat_no_bias_data =  data - bias_data\n",
    "        flat_no_bias.append(flat_no_bias_data)\n",
    "\n",
    "    # Subtracts dark data if it exists\n",
    "    if dark_filename:\n",
    "        dark_data = fits.getdata(dark_filename)\n",
    "        flat_no_bias_dark = []\n",
    "        count = 0\n",
    "        # Insures dark data is scaled by exposure time of flat\n",
    "        for data in flat_no_bias:\n",
    "            flat_no_bias_dark_data = data - (dark_data * flat_exptime[count])\n",
    "            flat_no_bias_dark.append(flat_no_bias_dark_data)\n",
    "            count += 1\n",
    "\n",
    "    # Sigma clip the bias-subtracted and or possibly dark-subtracted flats at sigma = 3\n",
    "    if dark_filename:\n",
    "        flat_sc = sigma_clip(flat_no_bias_dark, cenfunc='median', sigma=3, axis=0)\n",
    "    else:\n",
    "        flat_sc = sigma_clip(flat_no_bias, cenfunc='median', sigma=3, axis=0)\n",
    "\n",
    "    # Combine flat frames into a single median flat\n",
    "    median_flat = np.ma.mean(flat_sc, axis=0)\n",
    "    normalized_flat = median_flat / np.median(median_flat)\n",
    "\n",
    "    # Saves median flat frame to input name from 'median_flat_filename'\n",
    "    primary = fits.PrimaryHDU(data=normalized_flat.data, header=fits.Header())\n",
    "    hdul = fits.HDUList([primary])\n",
    "    hdul.writeto(median_flat_filename, overwrite=True)\n",
    "\n",
    "    # See code in create_median_bias for how to create a new FITS file\n",
    "    # from the resulting median flat frame.\n",
    "\n",
    "    return normalized_flat\n",
    "\n",
    "from astropy.io import fits\n",
    "\n",
    "def reduce_science_frame(\n",
    "    science_filename,\n",
    "    median_bias_filename,\n",
    "    median_flat_filename,\n",
    "    median_dark_filename,\n",
    "    reduced_science_filename=\"reduced_science.fits\",\n",
    "):\n",
    "    # Get data from each file type\n",
    "    sci_data = fits.getdata(science_filename)\n",
    "    bias_data = fits.getdata(median_bias_filename)\n",
    "    dark_data = fits.getdata(median_dark_filename)\n",
    "    flat_data = fits.getdata(median_flat_filename)\n",
    "\n",
    "    # Get the original header to be saved to the file\n",
    "    sci_header = fits.getheader(science_filename)\n",
    "    \n",
    "    # Get science data exposure time for dark current reduction\n",
    "    sci_exposure = fits.getheader(science_filename)['EXPTIME']\n",
    "\n",
    "    # Reduce science image\n",
    "    reduced_science = (sci_data - bias_data - (dark_data * sci_exposure)) / flat_data\n",
    "\n",
    "    # Save reduced science image\n",
    "    hdu = fits.PrimaryHDU(reduced_science, header=sci_header)\n",
    "    hdu.header['BUNIT'] = 'ADU'\n",
    "    hdu.header['COMMENT'] = 'Reduced science image: bias, dark, and flat corrected'\n",
    "    hdu.writeto(reduced_science_filename, overwrite=True)\n",
    "\n",
    "    # Return fully cleaned/reduced science image\n",
    "    return reduced_science\n",
    "\n",
    "def do_aperture_photometry(\n",
    "    image,\n",
    "    positions,\n",
    "    radii,\n",
    "    sky_radius_in,\n",
    "    sky_annulus_width,\n",
    "):\n",
    "    # Open up the reduced science image and get its data\n",
    "    image_data = fits.getdata(image)\n",
    "\n",
    "    # Set an index counter radii based off how many different coords \n",
    "    # are inside of positions\n",
    "    radii_count = 0\n",
    "\n",
    "    # Initialize astropy Qtable for storing output photometry info\n",
    "    phot_table = []\n",
    "\n",
    "    # Loop through every different object to do photometry of\n",
    "    for coords in positions:\n",
    "\n",
    "        # Create coords of the object\n",
    "        x_coord = coords[0]\n",
    "        y_coord = coords[1]\n",
    "\n",
    "        # Find aperture, annulus, and media background value\n",
    "        aperture = CircularAperture((x_coord, y_coord), r=radii[radii_count])\n",
    "        annulus = CircularAnnulus((x_coord, y_coord), r_in=sky_radius_in, r_out= sky_radius_in + sky_annulus_width)\n",
    "        back = (ApertureStats(image_data, annulus)).median\n",
    "\n",
    "        # Create table and prepare data\n",
    "        phot = aperture_photometry(image_data, aperture)\n",
    "        flux_raw = phot['aperture_sum'][0]\n",
    "        aperture_area = aperture.area_overlap(image_data)\n",
    "        flux_no_back = flux_raw - back * aperture_area\n",
    "\n",
    "        # Adds more info to table\n",
    "        phot['aperture_sum_no_back'] = flux_no_back\n",
    "        phot['id'] = radii_count\n",
    "        phot['radius'] = radii[radii_count]\n",
    "        phot['xcenter'] = x_coord\n",
    "        phot['ycenter'] = y_coord\n",
    "        phot['aperture_radius'] = sky_radius_in\n",
    "        \n",
    "        phot_table.append(phot)\n",
    "\n",
    "        # Update radii count\n",
    "        radii_count += 1\n",
    "        \n",
    "    # Creates a final table out of each individual table\n",
    "    final_phot_table = np.vstack(phot_table)\n",
    "\n",
    "    # Returns the table summing all objects to do aperture phot on\n",
    "    return final_phot_table\n",
    "\n",
    "def background_from_photometry(aperture_data, radii):\n",
    "    backgrounds = []\n",
    "    for row, r in zip(aperture_data, radii):\n",
    "        row_data = row[0]  # Unpack the single-element array\n",
    "        flux_raw = row_data[3]  # aperture_sum (Jy)\n",
    "        flux_no_back = row_data[4]  # aperture_sum_no_back (Jy)\n",
    "        aperture_area = np.pi * r**2  # Approximate area (pixels)\n",
    "        back = (flux_raw - flux_no_back) / aperture_area  # Jy/pixel\n",
    "        backgrounds.append(back)\n",
    "    mean_back = np.mean(backgrounds)\n",
    "    print(f\"Backgrounds: {backgrounds}, Mean: {mean_back:.2e} Jy/pixel\")\n",
    "    return mean_back\n",
    "  \n",
    "def subtract_background(image_file, background, output_file):\n",
    "    with fits.open(image_file) as hdul:\n",
    "        image_data = hdul[0].data\n",
    "        header = hdul[0].header\n",
    "        new_data = image_data - background\n",
    "        header['COMMENT'] = f'Subtracted background: {background:.2e} Jy/pixel'\n",
    "        hdu = fits.PrimaryHDU(new_data, header=header)\n",
    "        hdu.writeto(output_file, overwrite=True)\n",
    "        print(f\"Saved: {output_file}, Sample: {new_data[500:505, 500]}\")\n",
    "\n",
    "def compute_error_image(adu_files, science_file, zmag, exptime, gain=1.25, rdnoise=11.8):\n",
    "    error_squares = []\n",
    "    with fits.open(science_file) as hdul:\n",
    "        science_data = hdul[0].data.astype(np.float32, copy=False)\n",
    "    \n",
    "    for file in adu_files:\n",
    "        with fits.open(file) as hdul:\n",
    "            counts = hdul[0].data.astype(np.float32, copy=False)\n",
    "            counts_aligned, _ = aa.register(counts, science_data, max_control_points=100, detection_sigma=3)\n",
    "            noise_electrons = np.sqrt(np.maximum(counts_aligned / gain, 0) + rdnoise**2)\n",
    "            noise_counts = noise_electrons * gain\n",
    "            count_rate = counts_aligned / exptime\n",
    "            mask = count_rate > 0\n",
    "            flux = np.zeros_like(counts_aligned)\n",
    "            flux[mask] = 3631 * 10**(-0.4 * (zmag - 2.5 * np.log10(count_rate[mask])))\n",
    "            error_jy = np.zeros_like(counts_aligned)\n",
    "            error_jy[mask] = flux[mask] * (noise_counts[mask] / np.maximum(counts_aligned[mask], 1e-8))\n",
    "            error_squares.append(error_jy**2)\n",
    "    \n",
    "    combined_error = np.sqrt(np.nanmean(error_squares, axis=0))\n",
    "    return combined_error\n",
    "\n",
    "def create_error_image(science_file, adu_files, zmag, exptime, output_dir, gain=1.25, rdnoise=11.8):\n",
    "    error_data = compute_error_image(adu_files, science_file, zmag, exptime, gain, rdnoise)\n",
    "    \n",
    "    with fits.open(science_file) as hdul:\n",
    "        header = hdul[0].header\n",
    "    error_file = os.path.join(output_dir, f'error_{os.path.basename(science_file)}')\n",
    "    hdu = fits.PrimaryHDU(error_data, header=header)\n",
    "    hdu.header['BUNIT'] = 'Jy/pixel'\n",
    "    hdu.header['COMMENT'] = f'Noise image rdnoise={rdnoise} e-'\n",
    "    hdu.writeto(error_file, overwrite=True)\n",
    "    print(f\"Saved: {error_file}, Sample: {error_data[500:505, 500]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VUJoEasCAe52",
    "outputId": "8ab5822f-1493-4344-8e09-b357f8a65a6b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1376., 1391., 1389., ..., 1390., 1381., 1379.],\n",
       "       [1316., 1308., 1324., ..., 1314., 1309., 1314.],\n",
       "       [1319., 1325., 1328., ..., 1323., 1323., 1331.],\n",
       "       ...,\n",
       "       [1374., 1379., 1374., ..., 1375., 1380., 1380.],\n",
       "       [1369., 1381., 1374., ..., 1390., 1367., 1387.],\n",
       "       [1371., 1384., 1384., ..., 1384., 1373., 1374.]], dtype=float32)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creates a list out of all bias frames available in directory\n",
    "bias_list = glob.glob('Bias_BIN1_*.fits')\n",
    "# Creates a median bias from the bias frames in 'bias_list'\n",
    "create_median_bias(bias_list, 'median_bias.fit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h0U_80HeAnxM",
    "outputId": "e8ae40b9-612a-4062-878a-b5f369076f2c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "masked_array(\n",
       "  data=[[0.07799999713897705, 0.029333335161209107, 0.05400000214576721,\n",
       "         ..., 0.04733333289623261, 0.0753333330154419,\n",
       "         0.051999998092651364],\n",
       "        [0.04866666793823242, 0.08533333539962769, 0.04866666793823242,\n",
       "         ..., 0.2693333148956299, 0.0753333330154419,\n",
       "         0.08733333349227905],\n",
       "        [0.08399999737739564, 0.04333333671092987, 0.04466666579246521,\n",
       "         ..., 0.046666663885116574, 0.06399999856948853,\n",
       "         0.01133333370089531],\n",
       "        ...,\n",
       "        [0.09933333396911621, 0.058666670322418214, 0.07200000286102295,\n",
       "         ..., 0.05533333420753479, 0.027333331108093262,\n",
       "         0.07333334088325501],\n",
       "        [0.10866667032241821, 0.42800002098083495, 0.11333333253860474,\n",
       "         ..., 0.03999999761581421, 0.09533333778381348,\n",
       "         0.03866666555404663],\n",
       "        [0.08933333158493043, 0.028666666150093077, 0.04466666579246521,\n",
       "         ..., 0.40266666412353513, 0.07400000095367432,\n",
       "         0.07733333110809326]],\n",
       "  mask=[[False, False, False, ..., False, False, False],\n",
       "        [False, False, False, ..., False, False, False],\n",
       "        [False, False, False, ..., False, False, False],\n",
       "        ...,\n",
       "        [False, False, False, ..., False, False, False],\n",
       "        [False, False, False, ..., False, False, False],\n",
       "        [False, False, False, ..., False, False, False]],\n",
       "  fill_value=1e+20)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creates a list out of all dark frames present in the directory\n",
    "dark_list = glob.glob('Dark_BIN1_*.fits')\n",
    "# Creates a median bias using all dark frames inside 'dark_list' and the generated median bias frame\n",
    "# from the previous cell\n",
    "create_median_dark(dark_list, 'median_bias.fit', 'median_dark.fit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cq8MUeBoAq3s",
    "outputId": "1565be30-d0ca-4f9c-c5f1-d6b4420b9ab1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dberry/anaconda3/envs/pixedfit_testing/lib/python3.12/site-packages/numpy/core/fromnumeric.py:771: UserWarning: Warning: 'partition' will ignore the 'mask' of the MaskedArray.\n",
      "  a.partition(kth, axis=axis, kind=kind, order=order)\n"
     ]
    }
   ],
   "source": [
    "# Creates a list of flats for each associated filter present in the directory\n",
    "flat_list_g = glob.glob('domeflat_g_*.fits')\n",
    "flat_list_i = glob.glob('domeflat_i_*.fits')\n",
    "flat_list_r = glob.glob('domeflat_r_*.fits')\n",
    "flat_list_z = glob.glob('domeflat_z_*.fits')\n",
    "# Creates a median flat for every filter of 'flat_list' present above. Uses previously generated\n",
    "# median bias and median dark frame from above cells.\n",
    "median_flat_g = create_median_flat(flat_list_g, 'median_bias.fit', 'median_flat_g.fit', 'median_dark.fit')\n",
    "median_flat_i = create_median_flat(flat_list_i, 'median_bias.fit', 'median_flat_i.fit', 'median_dark.fit')\n",
    "median_flat_r = create_median_flat(flat_list_r, 'median_bias.fit', 'median_flat_r.fit', 'median_dark.fit')\n",
    "median_flat_z = create_median_flat(flat_list_z, 'median_bias.fit', 'median_flat_z.fit', 'median_dark.fit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "AkfxjYmpAtkz"
   },
   "outputs": [],
   "source": [
    "# Creates a list of ALL science images taken\n",
    "science_g = glob.glob('NGC 6946_g*.fits')\n",
    "science_i = glob.glob('NGC 6946_i*.fits')\n",
    "science_r = glob.glob('NGC 6946_r*.fits')\n",
    "science_z = glob.glob('NGC 6946_z*.fits')\n",
    "# Reduces each science frame depending on the filter used for the photo\n",
    "for i in range(len(science_g)):\n",
    "    reduce_science_frame(science_g[i], 'median_bias.fit', 'median_flat_g.fit', 'median_dark.fit', f'new_reduced_science_g{i}.fits')\n",
    "for i in range(len(science_i)):\n",
    "    reduce_science_frame(science_i[i], 'median_bias.fit', 'median_flat_i.fit', 'median_dark.fit', f'new_reduced_science_i{i}.fits')\n",
    "for i in range(len(science_r)):\n",
    "    reduce_science_frame(science_r[i], 'median_bias.fit', 'median_flat_r.fit', 'median_dark.fit', f'new_reduced_science_r{i}.fits')\n",
    "for i in range(len(science_z)):\n",
    "    reduce_science_frame(science_z[i], 'median_bias.fit', 'median_flat_z.fit', 'median_dark.fit', f'new_reduced_science_z{i}.fits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPTIME= 360.0\n",
      "ZMAG=  20.6181373634\n",
      "Saved flux-calibrated image as flux_calibrated_new_reduced_science_g0.fits\n",
      "Flux now in units of Jy, EXAMPLE: [2.11971020e-05 2.01517268e-05 2.18899600e-05 2.08516790e-05\n",
      " 2.19390882e-05]\n",
      "EXPTIME= 360.0\n",
      "ZMAG=  20.5949255573\n",
      "Saved flux-calibrated image as flux_calibrated_new_reduced_science_g1.fits\n",
      "Flux now in units of Jy, EXAMPLE: [1.97511223e-05 2.13290460e-05 2.08860515e-05 2.03192601e-05\n",
      " 1.91673608e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_499/2425285997.py:22: RuntimeWarning: invalid value encountered in log10\n",
      "  mag = header['ZMAG'] - 2.5 * np.log10(count_rate)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPTIME= 360.0\n",
      "ZMAG=  20.6281139709\n",
      "Saved flux-calibrated image as flux_calibrated_new_reduced_science_g2.fits\n",
      "Flux now in units of Jy, EXAMPLE: [1.98840017e-05 2.26794650e-05 1.87697121e-05 1.77446420e-05\n",
      " 2.10756611e-05]\n",
      "EXPTIME= 360.0\n",
      "ZMAG=  20.5921568932\n",
      "Saved flux-calibrated image as flux_calibrated_new_reduced_science_g3.fits\n",
      "Flux now in units of Jy, EXAMPLE: [1.89338753e-05 2.38436153e-05 2.25909299e-05 1.93856276e-05\n",
      " 2.30983815e-05]\n",
      "EXPTIME= 360.0\n",
      "ZMAG=  20.6361974692\n",
      "Saved flux-calibrated image as flux_calibrated_new_reduced_science_g4.fits\n",
      "Flux now in units of Jy, EXAMPLE: [1.86255968e-05 1.88853565e-05 2.12007376e-05 1.69450173e-05\n",
      " 1.75204885e-05]\n",
      "EXPTIME= 360.0\n",
      "ZMAG=  21.3949438559\n",
      "Saved flux-calibrated image as flux_calibrated_new_reduced_science_r0.fits\n",
      "Flux now in units of Jy, EXAMPLE: [2.50552449e-05 2.78727553e-05 2.57879233e-05 2.43233381e-05\n",
      " 2.42044564e-05]\n",
      "EXPTIME= 360.0\n",
      "ZMAG=  21.3427630018\n",
      "Saved flux-calibrated image as flux_calibrated_new_reduced_science_r1.fits\n",
      "Flux now in units of Jy, EXAMPLE: [4.21202184e-05 3.89860661e-05 3.05842696e-05 2.58110803e-05\n",
      " 2.40755157e-05]\n",
      "EXPTIME= 360.0\n",
      "ZMAG=  21.4484009501\n",
      "Saved flux-calibrated image as flux_calibrated_new_reduced_science_r2.fits\n",
      "Flux now in units of Jy, EXAMPLE: [2.04115874e-05 2.06591992e-05 2.02306442e-05 1.89156035e-05\n",
      " 1.90823772e-05]\n",
      "EXPTIME= 360.0\n",
      "ZMAG=  21.4250361904\n",
      "Saved flux-calibrated image as flux_calibrated_new_reduced_science_r3.fits\n",
      "Flux now in units of Jy, EXAMPLE: [1.91116411e-05 1.86969829e-05 1.89696017e-05 1.73631590e-05\n",
      " 1.85659825e-05]\n",
      "EXPTIME= 360.0\n",
      "ZMAG=  21.4189632158\n",
      "Saved flux-calibrated image as flux_calibrated_new_reduced_science_r4.fits\n",
      "Flux now in units of Jy, EXAMPLE: [1.73841573e-05 1.83707196e-05 1.86750678e-05 1.76769812e-05\n",
      " 1.87771742e-05]\n",
      "EXPTIME= 360.0\n",
      "ZMAG=  21.1448301098\n",
      "Saved flux-calibrated image as flux_calibrated_new_reduced_science_i0.fits\n",
      "Flux now in units of Jy, EXAMPLE: [2.77217044e-05 2.62722008e-05 2.69910794e-05 2.55596756e-05\n",
      " 2.75053321e-05]\n",
      "EXPTIME= 360.0\n",
      "ZMAG=  21.1327506334\n",
      "Saved flux-calibrated image as flux_calibrated_new_reduced_science_i1.fits\n",
      "Flux now in units of Jy, EXAMPLE: [2.35089859e-05 2.50917133e-05 2.40706058e-05 2.73630554e-05\n",
      " 2.60979276e-05]\n",
      "EXPTIME= 360.0\n",
      "ZMAG=  21.0381870205\n",
      "Saved flux-calibrated image as flux_calibrated_new_reduced_science_i2.fits\n",
      "Flux now in units of Jy, EXAMPLE: [2.71132594e-05 2.83708956e-05 2.47111449e-05 2.54641057e-05\n",
      " 2.75563754e-05]\n",
      "EXPTIME= 360.0\n",
      "ZMAG=  21.1077477877\n",
      "Saved flux-calibrated image as flux_calibrated_new_reduced_science_i3.fits\n",
      "Flux now in units of Jy, EXAMPLE: [2.41289595e-05 2.61791621e-05 2.34258050e-05 2.15005100e-05\n",
      " 2.58104466e-05]\n",
      "EXPTIME= 360.0\n",
      "ZMAG=  21.0773078045\n",
      "Saved flux-calibrated image as flux_calibrated_new_reduced_science_i4.fits\n",
      "Flux now in units of Jy, EXAMPLE: [2.54471811e-05 2.45221354e-05 2.55504809e-05 2.57141980e-05\n",
      " 2.53655494e-05]\n",
      "EXPTIME= 360.0\n",
      "ZMAG=  20.1181596039\n",
      "Saved flux-calibrated image as flux_calibrated_new_reduced_science_z0.fits\n",
      "Flux now in units of Jy, EXAMPLE: [6.14352509e-05 5.88444708e-05 6.38325755e-05 6.16805796e-05\n",
      " 5.99432425e-05]\n",
      "EXPTIME= 360.0\n",
      "ZMAG=  20.1478409128\n",
      "Saved flux-calibrated image as flux_calibrated_new_reduced_science_z1.fits\n",
      "Flux now in units of Jy, EXAMPLE: [6.59988713e-05 6.75519312e-05 6.72750725e-05 6.38102024e-05\n",
      " 6.28562963e-05]\n",
      "EXPTIME= 360.0\n",
      "ZMAG=  20.0758524159\n",
      "Saved flux-calibrated image as flux_calibrated_new_reduced_science_z2.fits\n",
      "Flux now in units of Jy, EXAMPLE: [6.83700038e-05 6.87040681e-05 6.09430853e-05 6.37544664e-05\n",
      " 6.03704768e-05]\n",
      "EXPTIME= 360.0\n",
      "ZMAG=  20.1328682408\n",
      "Saved flux-calibrated image as flux_calibrated_new_reduced_science_z3.fits\n",
      "Flux now in units of Jy, EXAMPLE: [5.63448834e-05 5.60008986e-05 5.77380685e-05 5.85253443e-05\n",
      " 5.62222169e-05]\n",
      "EXPTIME= 360.0\n",
      "ZMAG=  20.0095558639\n",
      "Saved flux-calibrated image as flux_calibrated_new_reduced_science_z4.fits\n",
      "Flux now in units of Jy, EXAMPLE: [6.34203418e-05 6.56346239e-05 6.16521538e-05 6.02544409e-05\n",
      " 5.89279010e-05]\n",
      "EXPTIME= 360.0\n",
      "ZMAG=  20.152012443\n",
      "Saved flux-calibrated image as flux_calibrated_new_reduced_science_z5.fits\n",
      "Flux now in units of Jy, EXAMPLE: [5.44000703e-05 5.46715261e-05 5.44140906e-05 5.39876238e-05\n",
      " 5.33305677e-05]\n",
      "EXPTIME= 360.0\n",
      "ZMAG=  20.1348027001\n",
      "Saved flux-calibrated image as flux_calibrated_new_reduced_science_z6.fits\n",
      "Flux now in units of Jy, EXAMPLE: [6.06780379e-05 6.13331810e-05 5.78966014e-05 5.77962416e-05\n",
      " 5.80616368e-05]\n"
     ]
    }
   ],
   "source": [
    "def adu_to_jy(list_files, curr_dir = '', reduced = False):\n",
    "    for file in list_files:\n",
    "        # Open up fits file\n",
    "        hdul = fits.open(curr_dir + file)\n",
    "        \n",
    "        # Extract data and header\n",
    "        data = hdul[0].data\n",
    "        header = hdul[0].header\n",
    "        \n",
    "        # Convert raw units into adu counts using BZERO and BSCALE\n",
    "        if not reduced:\n",
    "            adu_counts = header['BZERO'] + header['BSCALE'] * data\n",
    "            print(f'BZERO= {header['BZERO']}, BSCALE= {header['BSCALE']}.')\n",
    "        else:\n",
    "            adu_counts = data\n",
    "            \n",
    "        # Convert adu counts to a count rate using EXPTIME\n",
    "        count_rate = adu_counts / header['EXPTIME'] \n",
    "        print(f'EXPTIME= {header['EXPTIME']}')\n",
    "        \n",
    "        # Convert to a magnitude using ZMAG\n",
    "        mag = header['ZMAG'] - 2.5 * np.log10(count_rate)\n",
    "        print(f'ZMAG=  {header['ZMAG']}')\n",
    "        \n",
    "        # Convert to flux values in Jy using common zero point flux\n",
    "        zero_point_flux = 3631\n",
    "        flux = zero_point_flux * 10**(-0.4 * mag)\n",
    "        \n",
    "        # Save flux-calibrated image\n",
    "        output_filename = f'flux_calibrated_{file}'\n",
    "        hdu = fits.PrimaryHDU(flux, header=header)\n",
    "        hdu.header['BUNIT'] = 'Jy/pixel'\n",
    "        hdu.header['COMMENT'] = 'Flux-calibrated image in Janskys per pixel'\n",
    "        hdu.writeto(os.path.join(curr_dir, output_filename), overwrite=True)\n",
    "        print(f'Saved flux-calibrated image as {output_filename}')\n",
    "\n",
    "        # Close the open fits file just in case\n",
    "        hdul.close()\n",
    "        \n",
    "        # Print that the values have been converted and provide a small selection\n",
    "        print('Flux now in units of Jy, EXAMPLE:', flux[500:505,500])\n",
    "\n",
    "science_g = glob.glob(os.path.join('new_reduced_science_g*.fits'))\n",
    "science_i = glob.glob(os.path.join('new_reduced_science_i*.fits'))\n",
    "science_r = glob.glob(os.path.join('new_reduced_science_r*.fits'))\n",
    "science_z = glob.glob(os.path.join('new_reduced_science_z*.fits'))\n",
    "\n",
    "adu_to_jy(science_g, curr_dir, True)\n",
    "adu_to_jy(science_r, curr_dir, True)\n",
    "adu_to_jy(science_i, curr_dir, True)\n",
    "adu_to_jy(science_z, curr_dir, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TlUPjkt9GKRw",
    "outputId": "f49d0c8a-7a62-4875-c125-2e479ff7c1d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aligning flux_calibrated_new_reduced_science_g0.fits to flux_calibrated_new_reduced_science_g0.fits...\n",
      "Aligning flux_calibrated_new_reduced_science_g1.fits to flux_calibrated_new_reduced_science_g0.fits...\n",
      "Aligning flux_calibrated_new_reduced_science_g2.fits to flux_calibrated_new_reduced_science_g0.fits...\n",
      "Aligning flux_calibrated_new_reduced_science_g3.fits to flux_calibrated_new_reduced_science_g0.fits...\n",
      "Aligning flux_calibrated_new_reduced_science_g4.fits to flux_calibrated_new_reduced_science_g0.fits...\n",
      "Done aligning filter g.\n",
      "Aligning flux_calibrated_new_reduced_science_r0.fits to flux_calibrated_new_reduced_science_r0.fits...\n",
      "Aligning flux_calibrated_new_reduced_science_r1.fits to flux_calibrated_new_reduced_science_r0.fits...\n",
      "Aligning flux_calibrated_new_reduced_science_r2.fits to flux_calibrated_new_reduced_science_r0.fits...\n",
      "Aligning flux_calibrated_new_reduced_science_r3.fits to flux_calibrated_new_reduced_science_r0.fits...\n",
      "Aligning flux_calibrated_new_reduced_science_r4.fits to flux_calibrated_new_reduced_science_r0.fits...\n",
      "Done aligning filter r.\n",
      "Aligning flux_calibrated_new_reduced_science_i0.fits to flux_calibrated_new_reduced_science_i0.fits...\n",
      "Aligning flux_calibrated_new_reduced_science_i1.fits to flux_calibrated_new_reduced_science_i0.fits...\n",
      "Aligning flux_calibrated_new_reduced_science_i2.fits to flux_calibrated_new_reduced_science_i0.fits...\n",
      "Aligning flux_calibrated_new_reduced_science_i3.fits to flux_calibrated_new_reduced_science_i0.fits...\n",
      "Aligning flux_calibrated_new_reduced_science_i4.fits to flux_calibrated_new_reduced_science_i0.fits...\n",
      "Done aligning filter i.\n",
      "Aligning flux_calibrated_new_reduced_science_z0.fits to flux_calibrated_new_reduced_science_z0.fits...\n",
      "Aligning flux_calibrated_new_reduced_science_z1.fits to flux_calibrated_new_reduced_science_z0.fits...\n",
      "Aligning flux_calibrated_new_reduced_science_z2.fits to flux_calibrated_new_reduced_science_z0.fits...\n",
      "Aligning flux_calibrated_new_reduced_science_z3.fits to flux_calibrated_new_reduced_science_z0.fits...\n",
      "Aligning flux_calibrated_new_reduced_science_z4.fits to flux_calibrated_new_reduced_science_z0.fits...\n",
      "Aligning flux_calibrated_new_reduced_science_z5.fits to flux_calibrated_new_reduced_science_z0.fits...\n",
      "Aligning flux_calibrated_new_reduced_science_z6.fits to flux_calibrated_new_reduced_science_z0.fits...\n",
      "Done aligning filter z.\n"
     ]
    }
   ],
   "source": [
    "import astroalign as aa\n",
    "from astropy.io import fits\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def align_images(filter_name, num_images):\n",
    "    import astroalign as aa\n",
    "    from astropy.io import fits\n",
    "    import numpy as np\n",
    "    import os\n",
    "\n",
    "    aligned_dir = f\"aligned_{filter_name}\"\n",
    "    os.makedirs(aligned_dir, exist_ok=True)\n",
    "\n",
    "    ref_path = f\"flux_calibrated_new_reduced_science_{filter_name}0.fits\"\n",
    "    ref_data = fits.getdata(ref_path).astype(np.float32)\n",
    "\n",
    "    for i in range(num_images):\n",
    "        input_path = f\"flux_calibrated_new_reduced_science_{filter_name}{i}.fits\"\n",
    "        output_path = os.path.join(aligned_dir, f\"aligned_science_{filter_name}{i}.fits\")\n",
    "\n",
    "        print(f\"Aligning {input_path} to {ref_path}...\")\n",
    "\n",
    "        try:\n",
    "            data = fits.getdata(input_path).astype(np.float32)\n",
    "            header = fits.getheader(input_path)\n",
    "\n",
    "            aligned_data, _ = aa.register(data, ref_data)\n",
    "            fits.writeto(output_path, aligned_data, header, overwrite=True)\n",
    "\n",
    "        except aa.MaxIterError:\n",
    "            print(f\"Alignment failed for {input_path}. Skipping.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error with {input_path}: {e}\")\n",
    "\n",
    "    print(f\"Done aligning filter {filter_name}.\")\n",
    "# Example usage:\n",
    "align_images('g', 5)\n",
    "align_images('r', 5)\n",
    "align_images('i', 5)\n",
    "align_images('z', 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JhzfVIwNovXZ",
    "outputId": "f6903481-df13-4d0d-850f-16ccfccc1006"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved sigma-clipped combined image: combined_g.fits\n",
      "Saved sigma-clipped combined image: combined_r.fits\n",
      "Saved sigma-clipped combined image: combined_i.fits\n",
      "Saved sigma-clipped combined image: combined_z.fits\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from astropy.io import fits\n",
    "from astropy.stats import sigma_clip\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def sigma_clip_combine(filter_name):\n",
    "    aligned_dir = f'aligned_{filter_name}'\n",
    "    output_path = f'combined_{filter_name}.fits'\n",
    "\n",
    "    file_list = sorted(glob.glob(os.path.join(aligned_dir, f'flux_calibrated_aligned_science_{filter_name}*.fits')))\n",
    "    image_stack = []\n",
    "\n",
    "    for file in file_list:\n",
    "        data = fits.getdata(file).astype(np.float32)\n",
    "        image_stack.append(data)\n",
    "\n",
    "    stack = np.stack(image_stack)\n",
    "\n",
    "    # Apply sigma clipping along the stack axis (axis=0)\n",
    "    clipped = sigma_clip(stack, sigma=3.0, axis=0)\n",
    "\n",
    "    # Use mean of non-masked values\n",
    "    combined = np.ma.mean(clipped, axis=0)\n",
    "\n",
    "    header = fits.getheader(file_list[0])\n",
    "    fits.writeto(output_path, combined.filled(np.nan), header, overwrite=True)\n",
    "\n",
    "    print(f\"Saved sigma-clipped combined image: {output_path}\")\n",
    "\n",
    "sigma_clip_combine('g')\n",
    "sigma_clip_combine('r')\n",
    "sigma_clip_combine('i')\n",
    "sigma_clip_combine('z')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_499/1311142317.py:23: RuntimeWarning: All-NaN slice encountered\n",
      "  combined = np.nanmedian(np.stack(stack), axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved robust combined image: combined_g_robust.fits\n",
      "Saved robust combined image: combined_r_robust.fits\n",
      "Saved robust combined image: combined_i_robust.fits\n",
      "Saved robust combined image: combined_z_robust.fits\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from astropy.io import fits\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def robust_median_combine(filter_name, zero_threshold=True):\n",
    "    aligned_dir = f'aligned_{filter_name}'\n",
    "    output_path = f'combined_{filter_name}_robust.fits'\n",
    "\n",
    "    file_list = sorted(glob.glob(os.path.join(aligned_dir, f'aligned_science_{filter_name}*.fits')))\n",
    "    stack = []\n",
    "\n",
    "    for file in file_list:\n",
    "        data = fits.getdata(file).astype(np.float32)\n",
    "\n",
    "        # Optional: treat zeros as bad pixels (can change to any threshold)\n",
    "        if zero_threshold:\n",
    "            data[data <= 0] = np.nan  # also catches negative/noisy interpolated areas\n",
    "\n",
    "        stack.append(data)\n",
    "\n",
    "    # Use nanmedian to ignore NaNs (e.g., from zero-masking or WCS misalignment)\n",
    "    combined = np.nanmedian(np.stack(stack), axis=0)\n",
    "\n",
    "    header = fits.getheader(file_list[0])\n",
    "    fits.writeto(output_path, combined, header, overwrite=True)\n",
    "\n",
    "    print(f\"Saved robust combined image: {output_path}\")\n",
    "robust_median_combine('g')\n",
    "robust_median_combine('r')\n",
    "robust_median_combine('i')\n",
    "robust_median_combine('z')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RffLJBwIrTq9",
    "outputId": "f59901f7-e5c3-45de-b770-b64cb21ba4c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aligned combined_r_robust.fits → aligned_to_i_r.fits\n",
      "Aligned combined_g_robust.fits → aligned_to_i_g.fits\n",
      "Aligned combined_z_robust.fits → aligned_to_i_z.fits\n"
     ]
    }
   ],
   "source": [
    "import astroalign as aa\n",
    "from astropy.io import fits\n",
    "import numpy as np\n",
    "\n",
    "def align_to_reference(ref_path, target_path, output_path):\n",
    "    # Load reference image (g-band)\n",
    "    ref_data = fits.getdata(ref_path).astype(np.float32)\n",
    "\n",
    "    # Load target image (to align)\n",
    "    target_data = fits.getdata(target_path).astype(np.float32)\n",
    "    target_header = fits.getheader(target_path)\n",
    "\n",
    "    # Optional: mask non-positive values (can improve robustness)\n",
    "    target_data[target_data <= 0] = np.nan\n",
    "    ref_data[ref_data <= 0] = np.nan\n",
    "\n",
    "    # Align\n",
    "    aligned_data, _ = aa.register(target_data, ref_data)\n",
    "\n",
    "    # Save aligned image\n",
    "    fits.writeto(output_path, aligned_data, target_header, overwrite=True)\n",
    "    print(f\"Aligned {target_path} → {output_path}\")\n",
    "# Align r, i, z to g\n",
    "align_to_reference(\"combined_i_robust.fits\", \"combined_r_robust.fits\", \"aligned_to_i_r.fits\")\n",
    "align_to_reference(\"combined_i_robust.fits\", \"combined_g_robust.fits\", \"aligned_to_i_g.fits\")\n",
    "align_to_reference(\"combined_i_robust.fits\", \"combined_z_robust.fits\", \"aligned_to_i_z.fits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /mnt/d/pixedfit_testing/ARCSAT/error_bg_subtracted_combined_i.fits, Sample: [1.1934070e-06 1.1897234e-06 1.1907816e-06 1.1933917e-06 1.2012332e-06]\n",
      "Saved: /mnt/d/pixedfit_testing/ARCSAT/error_bg_subtracted_aligned_to_i_z.fits, Sample: [2.9392509e-06 2.9679113e-06 2.9440339e-06 2.9460655e-06 2.9613304e-06]\n",
      "Saved: /mnt/d/pixedfit_testing/ARCSAT/error_bg_subtracted_aligned_to_i_r.fits, Sample: [9.972019e-07 9.919970e-07 9.789127e-07 9.815377e-07 9.878520e-07]\n",
      "Saved: /mnt/d/pixedfit_testing/ARCSAT/error_bg_subtracted_aligned_to_i_g.fits, Sample: [1.5353215e-06 1.5201294e-06 1.5207438e-06 1.4990529e-06 1.5125976e-06]\n"
     ]
    }
   ],
   "source": [
    "positions = [(496, 131), (104, 302), (75,203), \n",
    "             (212, 681), (876, 216), (833, 635), (853, 864)]\n",
    "radii = [10, 11, 8, 8, 12, 8, 10]\n",
    "sky_radius_in = 10\n",
    "sky_annulus_width = 5\n",
    "\n",
    "aligned_reduced = ['combined_i.fits','aligned_to_i_z.fits','aligned_to_i_r.fits','aligned_to_i_g.fits']\n",
    "curr_dir = ''\n",
    "'''\n",
    "for file in aligned_reduced:\n",
    "    file_path = os.path.join(curr_dir, file)\n",
    "    aperture_data = do_aperture_photometry(file_path, positions, radii, sky_radius_in, sky_annulus_width)\n",
    "    mean_back = background_from_photometry(aperture_data, radii)\n",
    "    output_path = os.path.join(curr_dir, f'bg_subtracted_{file}')\n",
    "    subtract_background(file_path, mean_back, output_path)\n",
    "'''\n",
    "curr_dir = '/mnt/d/pixedfit_testing/ARCSAT/'\n",
    "science_files = [\n",
    "    'bg_subtracted_combined_i.fits',\n",
    "    'bg_subtracted_aligned_to_i_z.fits',\n",
    "    'bg_subtracted_aligned_to_i_r.fits',\n",
    "    'bg_subtracted_aligned_to_i_g.fits'\n",
    "]\n",
    "\n",
    "for science_file in science_files:\n",
    "    filter_name = 'i' if 'combined_i' in science_file else \\\n",
    "                  'z' if 'aligned_to_i_z' in science_file else \\\n",
    "                  'r' if 'aligned_to_i_r' in science_file else 'g'\n",
    "    adu_files = sorted(glob.glob(os.path.join(curr_dir, f'new_reduced_science_{filter_name}*.fits')))\n",
    "    with fits.open(adu_files[0]) as hdul:\n",
    "        zmag = hdul[0].header['ZMAG']\n",
    "        exptime = hdul[0].header['EXPTIME']\n",
    "    science_path = os.path.join(curr_dir, science_file)\n",
    "    create_error_image(science_path, adu_files, zmag, exptime, curr_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
